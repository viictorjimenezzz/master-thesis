\chapter{Robustness assessment}\label{chapter:robustness_assessment}

The fundamental goal of this project is to assess the suitability of posterior
agreement as a robust model selection criterion in the image classification setting.
This chapter will explore the properties of the PA kernel as a robustness metric in
adversarial and domain generalization settings. Evidence supporting its suitability against 
baseline accuracy measures will be provided, thus establishing PA as a reliable
algorithm selection criterion in these scenarios.

\section{PA as a robustness metric}\label{sec:results_robustness}

\subsection{Empirical behaviour}

Starting from the simplest possible setting, we will explore the behavior of the metric 
under different levels of sample mismatch. More specifically, we will assess the performance 
of perfect, random, and constant binary classifiers by manipulating a 
Bernoulli sample and simulating different levels of prediction confidence.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/empirical/artificial_acc_final.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/empirical/artificial_logPA_final.png}
    \end{subfigure}
    \caption{Evolution of performance and robustness for the three classifiers}
    \label{fig:empirical_plot}
\end{figure}

A $N=1000$ Bernoulli sample was generated with a symmetrical confidence level 
of prediction $\pm \Delta/2$. For $p=0.5$, the metric achieves its minimum 
value $N \log{1/2}$ for the random classifier, as $\beta = 0$, and tends to its maximum
value of $0$ as $\beta \longrightarrow \infty$.

\begin{table}[H]
    \centering
    \begin{tabular}{lccc}
    Classifier & Accuracy & $\beta$ & PA \\
    \midrule
    Perfect   & 1.000     & 12.331  & $-0.0088218$ \\
    Constant  & 0.525     & 12.331  & $-0.0088218$ \\
    Random    & 0.516     & 0.000   & $-693.14$    \\
    \bottomrule
    \end{tabular}
    \caption{Comparison of classifier performance metrics for $p = 0.5$.}
    \label{tab:empirical_table}
\end{table}

The random classifier sample was generated by permuting the original so 
that the number of mismatched observations depends on the bernoulli probability $p$. 
As we can see, the theoretical miniumum value  is obtained only after
a certain perturbation threshold has been reached. This illustrates the trade-off
navigated during the kernel optimization, in which matching samples penalize the 
metric value the lower the value of $\beta$ is, whereas mismatching samples will
penalize the overall value the further from zero $\beta$ is. Given the highly non-linear
nature of the logarithm in the interval $[0,1]$, the metric will penalize 
disagreement much more than agreement, as shown in Figure \ref{fig:empirical_plot}.
A truly random classifier (i.e. balanced on the two classes) would yield the minimum
PA value for any possible original sample, as shown in the blue dashed line.\\

One of the most relevant differences between posterior agreement and any accuracy-based
measure, as discussed in previous chapters, is the fact that its assessment is based on
the whole probabilistic output of the model, and therefore can be used as a measure of
confidence in the predictions. The prediction confidence, expressed as a difference in 
the unnormalized log-odds (commonly known as logits), is very informative with regard 
to the quality of the model, as we can intuitively infer that the latent 
space represented by a high-confidence model encodes a better set of features 
to discriminate observation classes than one with a lower prediction confidence. \\

For instance, when comparing two models of similar predictive power but at different 
confidence levels, maximum posterior agreement will be achieved with a higher 
$\beta^{*}$ for the model that tends to yield more flattened distributions. This
information is especially valuable in the covariate shift setting, given that robust
models that rely on less accessible sets of features will most likely yield conservative
predictions on in-distribution samples, but at the same time keep a high prediction
power on out-of-distribution observations. \\

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/empirical/nonrob_met=betas_hue=ldiff.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/empirical/nonrob_met=logPA_hue=ldiff.png}
    \end{subfigure}
    \caption{Evolution of PA kernel optimization under different levels of prediction 
    confidence. An illustration of the original log-odds and its associated posterior distribution
    can be found in Appendix \ref{subsec:appendix_empirical_behaviour}.}
    \label{fig:prediction_confidence}
\end{figure}

These and other results (see Appendix \ref{subsec:appendix_empirical_behaviour}) indicate
that the PA kernel behaves as expected and is highly informative of the generalization
capabilities of the model, provided that the nature of the randomness existing
between $\bm{x}^\prime$ and $\bm{x}^{\prime \prime}$ is known.

\subsection{Robustness assessment to sampling randomness}

The results obtained with artificial samples motivate the exploration of more realistic
scenarios. In general, the PA metric is expected to capture the generalization capabilities
of any model yielding probabilistic predictions, regardless of the task at hand. This
already represents an incredible advantage from an epistemological perspective, as we
can argue that the metric is agnostic of the underlying mechanism that generated the data
and even to the nature of the data itself. \\

In order to verify this claim, we will start by evaluating the robustness of two
different classifier models in two different domains under increasing levels of 
white noise perturbation. This particular setting, even if highly artificial, is 
relevant in any classification context, as it represents general measure of the 
quality of the features learned by the model. The presence of white noise, at least 
at low levels, does not perturb the set of features that define a particular class from
a human perspective, and should therefore not perturb very significantly the
predictions yielded by the model. \\

HERE THE PLOT OF THE LEVENSHTEIN DISTANCE PLOT HERE. \\

A sentiment classifier analysis. In this case, the random nature of the noise perturbations
is shown to exploit specific vulnerabilities of language models, which paradoxically 
have been shown to be robust to more highly crafted perturbations, such as changes in
language or replacement of complete words. \\

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/GAUSSIAN_logPA_eps_single.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/GAUSSIAN_acc_pa_eps_single.png}
    \end{subfigure}
    \caption{PA and accuracy of CIFAR10 classification for increasing levels of white noise intensity.}
    \label{fig:gaussian_noise}
\end{figure}

In this second example, a 10.000 observation sample of CIFAR10 images was perturbed
with white noise at different levels of intensity. The magnitude of the perturbation
is expressed in the same terms as those of an adversarial 
attack (see Section \ref{sec:adversarial_setting}) for further 
reference, but translate to using $\sigma = 3 \ell_\infty$, as 99.73\% of the
total mass of the gaussian distribution lies within the interval $\pm 3\sigma$. \\

As expected, PA is highly sensitive to the presence of white noise,
and is able to capture the generalization capabilities of the model in a much more
informative way than accuracy. We can obtain a higher understanding of the degree of
sensitivity of the metric if we adjust the perturbation so that if only affects
a certain ratio of observations. \\

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/empirical/betas.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/empirical/logpas.png}
    \end{subfigure}
    \caption{PA kernel optimization in the CIFAR10 gaussian noise setting for different ratio
    of perturbed samples. Perturbation magnitude is $\ell_\infty$ = 32 / 255.}
    \label{fig:gaussian_optimization}
\end{figure}

As expected, $\beta$ tends to infinity in the unperturbed case, and converges
quickly to its optimal value in the rest of cases, even if the sample size is considerably
large and memory-intensive. The decay in the PA value is less pronounced the higher
fraction of perturbed samples are there, as we observed in the artificial setting, which
is consistent with the concept of robustness itself, as it already approaches
the lower bound for these kinds of perturbation even when the whole sample has yet not
been perturbed. \\

\section{Adversarial setting}\label{sec:results_adversarial}

The first scenario in which covariate shift robustness will be tested is the
adversarial setting. This setting serves as an archetypal use case for any robustness 
metric, given that adversarial perturbations are deliberately generated to mislead the
model, and any robustness score will ultimately be driven by the effectiveness of the 
attack. In particular, PA should be highly informative about the defensive capabilities 
of models, as the posterior distribution over the hypothesis class will shift 
significantly in the presence of adversarial perturbations. This section aims to validate 
this claim and provide deeper insights into the nature of the metric. \\

It is important to note that adversarial perturbations constitute an
intermediate instance between sampling randomness and distribution shift. 
On the one hand, they emulate a sampling variation that appears 
as an outlier under the model's representation of the true class, even if
the source of variability is completely artificial. On the other
hand, samples are known to contain the set of features that should 
align with the inductive bias of the model, and so the model's ability to 
distillate those features is in question. In practice, we are evaluating the 
quality of the complex discriminator function defining a basin of stability
around original samples, and for that no deep understanding of the nature of 
the randomness of the samples or the features they encode is needed.\\

This interpretation is aligned with the measure provided by accuracy-based metrics, 
because adversarial samples are not expected to contain any relevant features of other
classes or express any accountable source of randomness, but instead exploit specific
vulnerabilities of models to alter the position of the maximum of 
the posterior ditribution. A greater posterior overlap will still
indicate higher robustness to attacks, regardless of the nature of the model or the
attack, but optimal posteriors are expected to converge to very peaked gibbs
distributions centered at the predicted class, reducing the interpretability of PA
to that of accuracy. \\

In order to explore these claims, robustness and performance results will be
provided through the adversarial fidelity ratio (AFR) value and compared to those
yielded by PA. The AFR computed with the true class labels will be used as a baseline
of model performance, whereas the AFR computed with the predicted class label
will be a reference for robustness, as it aligns with the aforementioned
interpretation.

\begin{definition}[Adversarial fidelity ratio]
    Let $\bm{\hat{y}^\prime}, \bm{\hat{y}^{\prime\prime}} \in \mathcal{Y}^N$ be the predicted class 
    labels for $\bm{x}^\prime$ and $\bm{x}^{\prime \prime}$, respectively, 
    and $\bm{y}\in \mathcal{Y}^N$ the true labels. Let $\operatorname{ACC}$ be the standard accuracy 
    metric, as defined in Section \ref{sec:robustness_to_covariate_shift}.
    The adversarial fidelity ratio (AFR) is expressed as

    $$
    \begin{aligned}
        \operatorname{AFR (T)} &= \operatorname{ACC}(\bm{\hat{y}^{\prime \prime}}, \bm{y}), \\
        \operatorname{AFR (P)} &= \operatorname{ACC}(\bm{\hat{y}^{\prime \prime}}, \bm{\hat{y}^{\prime}}).
    \end{aligned}
    $$

\end{definition}

The results provided in this section have been obtained using the CIFAR10 dataset
\cite{krizhevskyLearningMultipleLayers},
which is widely regarded as a standard benchmark for robustness evaluation. CIFAR10 is
a balanced dataset containing 60.000 coloured 32 $\times$ 32 pixel images belonging to 10
different classes. We will consider a pre-trained WideResNet-28-10 as a baseline, undefended
model and compare it to some state-of-the-art robust models provided by the 
RobustBench \cite{croceRobustBenchStandardizedAdversarial2021a}
library under PGD \cite{madryDeepLearningModels2019}
and FMN \cite{pintorFastMinimumnormAdversarial2021}
attacks, both run for a thousand steps (see Section \ref{sec:adversarial_setting}). 
The PGD attack power will be specified in terms of $\ell_\infty$, which corresponds
to the maximum perturbation allowed for each pixel. This is consistent with the characterization
of adversarial perturbation given in the previous chapter, as every perturbation will be bounded
to the region defined by $\mathbf{B}_\infty^{\ell_{\infty}} (x)$.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/adv_unperturbed_framed.png}
        \caption{Original}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/adv_pgd_framed.png}
        \caption{PGD, $\ell_\infty$ = 36/255}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/adv_fmn_framed.png}
        \caption{FMN}
    \end{subfigure}
    \caption{Original and adversarially-perturbed CIFAR10 sample of class \texttt{horse}. Both perturbations succeed
    at misleading an undefended, pre-trained WideResNet-28-10 net.}
\end{figure}

Besides the maximum norm allowed for each perturbation, we are also interested in evaluating 
the sensitivity of our robustness measure to the ratio of perturbed samples in the dataset, 
also known as adversarial ratio (AR). The final adversarial dataset $\bm{x}''$ will be generated as

$$
\bm{x}'' := \operatorname{AR} \bm{x}'' + (1 - \operatorname{AR}) \bm{x}',
$$

where $\bm{x}'' = \bm{x}' + \bm{\Delta}$, as per Definition \ref{def:adversarial_perturbation}.
This incremental expansion of the attack is particularly relevant for PA, as we would initially 
expect it to behave non-linearly with respect to $\operatorname{AR}$ and converge faster to
the the $\operatorname{AR}=1$ robustness value than any accuracy-based metric, in light of the results obtained
in the previous section. We can quantify the model discriminability over increasing $\operatorname{AR}$
by computing the adversarial ratio gap $\Delta \operatorname{AR}$.\\

\begin{definition}[Adversarial ratio gap]
    Let $\gamma_+$ and $\gamma_-$ be two models, not necessarily different. Let $\operatorname{AR}_{+}$ and 
    $\operatorname{AR}_{-}$ be the adversarial ratio values such that

    $$
    \operatorname{PA}^{\gamma_+} \bigg|_{\operatorname{AR}_{+}} = \operatorname{PA}^{\gamma_-}\bigg|_{\operatorname{AR}_{-}}
    $$

    the aversarial ratio gap ($\Delta \operatorname{AR}$) is obtained as

    $$
    \Delta\operatorname{AR} = \operatorname{AR}_{+} - \operatorname{AR}_{-}. 
    $$
\end{definition}

Before delving into the results, it is worth exploring the immediate consequences
of the previous claim, namely the fact that the maximum posterior agreement will
be achieved when gibbs distributions are highly peaked on the predicted 
class, at least for moderately aggressive attacks. This is because most adversarial 
samples will not succeed at misleading the model and thus drive the inverse temperature to
infinity. The divergence of $\beta^{*}$ is only limited by the set of misleading adversarial 
samples, that for being perturbed from the original class are still expected to assign a
significant confidence to the original prediction, even if not the maximum anymore.
Table \ref{tab:entropy_gibbs} illustrates this claim by showing that $\beta^{*} > 1$ 
for all robust models, resulting in a substantial decrease of the entropy between initial and 
optimal posteriors. \\

\begin{table}[H]
    \centering
        \begin{tabular}{l|rr|rr}
        Defense & $\beta^{*}_{\text{PGD}}$ & $\Delta H_{\text{PGD}}$  & $\beta^{*}_{\text{FMN}}$ & $\Delta H_{\text{FMN}}$ \\
        \midrule
        {\color{tab:orange} \textbf{Undefended}} & 0.78 & 0.048 & 0.65 & 0.10\\
        {\color{tab:blue} \textbf{Engstrom et al.}} & 15.63 & -1.204 & 2.59 & -0.71\\
        {\color{tab:green} \textbf{Athalye et al.}} & 35.48 & -3.049 & 19.84 & -2.13 \\
        {\color{tab:red} \textbf{Wong et al.}} & 15.46 & -1.229 & 4.59 & -0.96\\
        {\color{tab:purple} \textbf{Addepalli et al.}} & 15.89 & -2.023 & 6.08 & -1.71 \\
        {\color{tab:brown} \textbf{Wang et al.}} & 11.24 & -1.833 & 2.53 & -1.41\\
        \bottomrule
        \end{tabular}
        \caption{
        Entropy difference $\Delta H = H(\beta^{*}) - H(\beta)$
        for different models, obtained for FMN and $\ell_\infty$ = 8/255 
        PGD attacks, both at $\operatorname{AR} = 1$. Entropy values are 
        estimated using the average posterior distribution over correctly classified
        samples, which constitute the largest proportion of the dataset.
        Figures \ref{fig:pgd_distributions_undefended}-\ref{fig:pgd_distributions_bpda}
        show the initial and optimal average posteriors from which these values
        were computed.
        }
        \label{tab:entropy_gibbs}
\end{table}

This realization allows us to break down the dataset into subsets of observations
that contribute to the final PA value in different ways, and therefore improve
the interpretation of the resulting robustness measurement.
For a start, a robust model should be expected to correctly classify most of the 
original samples with high confidence, as they
contain the discriminative features that define each class. Also in the original 
dataset, lack of generalization to sampling randomness should be penalized for lowering
the confidence in the predicted class. Regarding adversarial samples, a clear
distinction between robust and non-robust models should be made based on the success 
rate of perturbations and the confidence attributed to misleading predictions. Adversarial 
perturbations on samples originally misclassified will not be of much interest,
as the effect on prediction confidence should not be as significant as in
the correctly classified ones. An interpretable expression for PA in the adversarial
setting can be obained by approximating the optimal posterior for each of these
groups of observations. \\

\begin{theorem}[Approximated PA in the adversarial setting]
    Let $\Xi_{\text{ERR}}$, $\Xi_{\text{MIS}}$ and $\Xi_{\text{ADV}}$ be the approximated robustness
    contributions of correctly classified original samples, misclassified original samples,
    and misleading adversarial samples, respectively. Then, we can express

    $$
    \operatorname{PA} \approx \Xi_{\text{SAM}} + \Xi_{\text{ADV}} = \Xi_{\text{ERR}} + \Xi_{\text{MIS}} + \Xi_{\text{ADV}}
    $$

    where
    $$
    \begin{aligned}
        &\Xi_{\text{ERR}} = N \tau \rho \log \left( 1 - 2\delta_{\text{ERR}} \right), \\
        &\Xi_{\text{MIS}} = N (1- \tau) \rho \log \left( 1 - 2\delta_{\text{MIS}} \right), \\
        &\Xi_{\text{ADV}} = N \tau (1 - \rho) \log \delta_{\text{ADV}},
    \end{aligned}
    $$

    where $\tau$ is the accuracy of the model in the original data and $\rho \equiv$ AFR (P).
    Variables $\delta_{\text{ERR}}$, $\delta_{\text{MIS}}$ and $\delta_{\text{ADV}}$ account for the 
    average probability assigned to
    classes other than the predicted class for the three aforementioned cases 
    (see illustration in Figure \ref{fig:appendix_adv_illustration}). $\Xi_{\text{SAM}}$ aggregates
    the first two terms and will be interpreted as the sampling randomness contribution.
    \label{thm:approximated_pa}
\end{theorem}
\begin{proof}
    See Apendix \ref{sec:appendix_results_adversarial}.
\end{proof}

Figures \ref{fig:appendix_adversarial_approx_pa_pgd} and \ref{fig:appendix_adversarial_approx_pa_fmn}
compare the true and approximated PA values under increasing adversarial ratio for
PGD and FMN attacks, respectively. It is clear that penalizations are overestimated, 
given that the average posterior probability was
used and differences by defect are more significant than those by excess due to
the nonlinear nature of the logarithm in the range $[0,1]$.
Besides, $\beta^{*}$ is fixed to its lowest possible value; that
is, when $\operatorname{AR} = 1$. This makes the approximation on the FMN attack less
reliable for smaller adversarial ratio settings, as $\beta^{*}$ decreases significantly
due to the effectiveness of the attack. \\

Nevertheless, the relative differences in the approximated PA values are consistent
with the true values, and the ranking of the models is largely preserved across different
adversarial ratios, especially for $\operatorname{AR} = 1$. For that reason, the
interpretability provided by the approximated PA expression will illustrate
the results, and will be used to better characterize the source of robust and unrobust behaviour
observed in the different models. \\


\subsection{Adversarial robustness assessment with PA}

The first results presented correspond to PGD attacks with different attack power 
$\ell_\infty$, namely 8/255, 16/255 and 32/255, for increasing ratio of 
perturbed samples in the CIFAR10 dataset.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/PGD_0.0314_combo.png}
    \end{subfigure}

    \vspace{1em}

    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/PGD_0.0627_combo.png}
    \end{subfigure}

    \vspace{1em}

    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/PGD_0.1255_combo.png}
    \end{subfigure}

    \caption{PA, AFR(T) and the AFR variation against increasing adversarial ratio at different
    perturbation norm bounds. The aforementioned undefended net and several RobustBench
    robust models are considered under a 1000 step PGD attack.}
    \label{fig:six_figures_pa_adv}
\end{figure}

At first glance, it is clear that PA is able to discriminate robust models from
the {\color{tab:orange} \textbf{Undefended}} one, which is shown to significantly
decrease its performance with increasing adversarial ratio and attack power. As expected, 
the rate at which its performance decreases is higher
the more powerful the attack is, since a greater percentage of samples are able to
mislead its predictions. \\

From both PA and AFR stems the fact that {\color{tab:green} \textbf{Athalye et al.}}
is significantly less robust to PGD attacks than its RobustBench counterparts, as 
its performance decreases
way more significantly with increasing $\operatorname{AR}$. It is interesting to see, however,
than the rate at which its performance decreases is inversely proportional to the 
attack power, which indicates that the principle by which robustness is achieved
is more effective for large-norm perturbations. \\

A fundamental difference between these two models, that cannot be
inferred from a purely performance-based metric, is the nature of the shift in
the probabilistic output of the model, which is the source of the robust and non-robust
behaviour observed. Figure \ref{fig:unrobust_posterior_short_pgd} \textbf{(right)}
shows the optimal $\beta^{*}$ value for each model, which is an indication of
the entropy of the posterior distribution and discriminates the two non-robust
models from the rest and from each other. The {\color{tab:orange} \textbf{Undefended}}
model provides overconfident predictions that maximize disagreement in misleading and 
misclassified samples, whereas {\color{tab:green} \textbf{Athalye et al.}} provides 
uncertain predictions that minimize disagreement in adversarial samples but have 
the opposite effect in correctly classified ones. This insight clarifies the 
unintuitive behaviour observed earlier, by which {\color{tab:green} \textbf{Athalye et al.}} 
robustness value decreases at a lower rate with increasing attack power, despite maintaining a
constant decrease in performance of $\Delta \operatorname{AFR} \sim 0.1$. \\

Figure \ref{fig:unrobust_posterior_short_pgd} \textbf{(left)} illustrates
the previous reasoning by displaying the average posterior
probability assigned to the predicted class by each model, conditioned on the type
of prediction assigned. This discrimination yields three groups of observations,
namely original samples that are correctly classified by the robust model, 
original samples that are misclassified, and perturbed samples that, having their 
associated unperturbed sample been correctly classified, 
have been able to mislead the model. These three cases are relevant from the 
adversarial robustness perspective,  as they illustrate the trade-off between high-confident 
original predictions and adversarial vulnerability, which has been already stated in 
previous chapters. {\color{tab:brown} \textbf{Wang et al.}}
acts as a reference for an ideal robust behaviour, in which original samples are
predicted with high confidence and adversarially misleading predicted labels are 
only slightly more likely than the rest. Equivalent representations for the remaining
models can be found in Figure \ref{fig:appendix_adversarial_distribution_pgd}.\\

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/bpda_wang_undefended_beta_pgd.png}
    \end{subfigure}
   
    \caption{(\textbf{left}) Average posterior probability of the predicted class for 
    correctly classified original samples, misclassified original samples, and 
    misleading adversarial samples, respectively. (\textbf{right}) Optimal $\beta^{*}$ value for each model.
    Results obtained through a PGD attack with $\ell_\infty = 8 / 255$.}
    \label{fig:unrobust_posterior_short_pgd}
\end{figure}

With respect to robust models, we observe a significant difference in the 
discriminative power of PA and accuracy-based metrics that does not immediately
derive from the informativeness of the optimal posterior. As remarked before, AFR (P)
constitutes our baseline robustness metric, as by definition represents the ratio
of predictions that remained constant under adversarial perturbations, and
therefore ranks models by their predictive capabilities against these attacks. The
value of $\Delta$AFR aligns with that definition, and discriminates robust models
by a very thin margin, selecting {\color{tab:brown} \textbf{Wang et al.}} as
the best. Further analysis on PA is needed to understand the source of this 
discrepancy, as for instance why {\color{tab:purple} \textbf{Addepalli et al.}} model
is attributed a significantly lower value than the remaining robust models
under a $\ell_\infty$ = 8/255 PGD attack, despite displaying a similar decrease in performance. \\

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.37\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/PGD_logPA_eps.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.59\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/PGD_AFR_true_eps_diff.png}
    \end{subfigure}
    \caption{PA, AFR(T) and the AFR variation against increasing attack power for  $\operatorname{AR} = 1$. 
    The aforementioned undefended net and several RobustBench robust models are considered
    under a 1000 step PGD attack.}
    \label{fig:pgd_eps}
\end{figure}

Finally, Figure \ref{fig:pgd_eps} shows that PA is also discriminative with respect to
increasing attack power, expressed through the maximum allowed $\ell_\infty$ norm. 
As mentioned earlier, PA values are heavily aligned with the performance
decrease of the models under a specific attack power, but the observed decrease in PA 
under increasing $\ell_\infty$ is much more significant that the decrease 
in performance. This can be explained by the fact that the metric is sensitive 
to the overall posterior shift and not only the position of the maximum. When increasing 
the attack power, confidence in the predicted class will decrease in general, 
even when the sample does not succeed at misleading the model, and therefore the
overall overlap between posteriors will be reduced even at comparable performance 
levels. This observation further illustrates the independent discriminability power
offered by PA (see Properties \ref{properties:robustness}), which constitutes the 
cornerstone argument of this work. \\

In order to widen the scope of the analysis, analogous results will be obtained for
FMN attacks, which are expected to be more effective than PGD attacks for being 
unbounded, which translates into an overall decrease in $\beta^{*}$ (see 
Table \ref{tab:entropy_gibbs}). Figure \ref{fig:adv_fmn_pa_afr} shows the evolution 
of PA against increasing adversarial ratio for the same models, and compares
it with the assessment provided by AFR.


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.39\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/FMN_logPA.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.59\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/FMN_1000_AFR_true.png}
    \end{subfigure}
    \caption{PA, AFR(T) and the AFR variation against increasing adversarial ratio. 
    The aforementioned undefended net and several RobustBench robust models are considered 
    under a 1000 step FMN attack.}
    \label{fig:adv_fmn_pa_afr}
\end{figure}


As expected, the effectiveness of the FMN attack is superior to that of PGD attacks, as
the decrease in performance is substantially more significant for all models, especially the ones
previously considered robust. It is likely that these models have been defended with a
compression strategy that succeeds at filtering out small perturbations, which are
the ones employed by FMN, and for that reason maintain their performance at low
adversarial ratio values \cite{dasKeepingBadGuys2017}.
In particular, {\color{tab:green} \textbf{Athalye et al.}} remains maximally robust
until at least 40\% of the samples are perturbed, at which point the defensive strategy
is neutralized and a constant fraction of the additional perturbed samples succeeds at
misleading the model, which translates into a linear decrease in performance and PA. \\

PA proves to be very discriminative among robust models and to represent the 
phase transition entailed by the collapse of the defense strategy better than AFR does, which can be
observed in more detail in Figure \ref{fig:appendix_adversarial_afrpred_fmn}. A significative
result is that PA is not so directly aligned with $\Delta$AFR, in contrast to
the PGD case, which shows again that the decrease in performance is not the main driver of
the robustness assessment provided by PA, but instead can be interpreted as a consequence of
a misalignment in the posterior distributions of adversarial samples, which are the ones driving
the metric after the $\operatorname{AR}$ thresold is reached. \\

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/bpda_wang_undefended_beta_fmn.png}
    \end{subfigure}
   
    \caption{(\textbf{left}) Average posterior probability of the predicted class for 
    correctly classified original samples, misclassified original samples, and 
    misleading adversarial samples, respectively. (\textbf{right}) Optimal $\beta^{*}$ value for each model.
    Results obtained through a FMN attack.}
    \label{fig:unrobust_posterior_short_fmn}
\end{figure}

Figure \ref{fig:unrobust_posterior_short_fmn} gives insight into the probabilistic output
of the model and the informativeness of the optimal posterior for the {\color{tab:orange} \textbf{Undefended}}, 
{\color{tab:green} \textbf{Athalye et al.}} and {\color{tab:brown} \textbf{Wang et al.}} models, in
analogous way to the PGD experiments. The first two models display a very similar behaviour for 
$\beta = 1$, but optimal posteriors are less informative due to the increased number of misleading
samples, which translates into a smaller $\beta^{*}$. The response of 
the {\color{tab:brown} \textbf{Wang et al.}} model further illustrates the higher effectiveness of
FMN attacks, as adversarial perturbations are on average more misleading than outlier samples in
the original dataset, which did not occur in the PGD case. Analogous representations for the remaining
models can be found in Figure \ref{fig:appendix_adversarial_distribution_fmn}, which show
that the {\color{tab:purple} \textbf{Addepalli et al.}} is the only robust model that maintains
the same behaviour under both attacks.\\

Overall, we recognize that PA has a higher discriminative power than AFR, especially
considering the evolution of each metric over increasing adversarial ratio, as
seen in Figures \ref{fig:six_figures_pa_adv} and \ref{fig:adv_fmn_pa_afr}. 
In particular, Figures \ref{fig:appendix_adversarial_afrpred_pgd} and \ref{fig:appendix_adversarial_afrpred_fmn}
compare the evolution of PA with that of AFR (P), which is the baseline metric for robustness,
and show the susceptibility of the latter to dataset variability. This is an important
consideration, as PA not only improves the discriminability in terms of the scale of
the differences between models, but also provides a more stable assessment across varying
levels of perturbed sample presence, under which AFR (P) exhibits significant fluctuations
that alter the ranking of the models at every step, as illustrated in Table \ref{tab:pa_afrpred_comparison_table}. \\

\begin{table}[h]
    \centering
    \begin{tabular}{l|cc|cc|cc|cc}
    \multirow{2}{*}{Defense} & \multicolumn{2}{c|}{$\operatorname{AR}$ = 0.0} & \multicolumn{2}{c|}{$\operatorname{AR}$ = 0.2} & \multicolumn{2}{c|}{$\operatorname{AR}$ = 0.4} & \multicolumn{2}{c}{$\operatorname{AR}$ = 0.6} \\
    & PA & $\operatorname{AFR}_{\text{P}}$ & PA & $\operatorname{AFR}_{\text{P}}$ & PA & $\operatorname{AFR}_{\text{P}}$ & PA & $\operatorname{AFR}_{\text{P}}$ \\
    \midrule
    {\color{tab:purple} \textbf{Addepalli et al.}} & \textbf{-169.45} & 1 & \textbf{-172.51} & \textbf{0.9956} & \textbf{-175.46} & 0.9920 & \textbf{-177.63} & 0.9896 \\
    {\color{tab:red} \textbf{Wong et al.}} & -91.47 & 1 & -97.68 & 0.9960 & -102.90 & 0.9920 & -109.21 & \textbf{0.9876} \\
    {\color{tab:blue} \textbf{Engstrom et al.}} & -89.20 & 1 & -94.21 & 0.9964 & -104.61 & \textbf{0.9904} & -110.34 & 0.9888 \\
    {\color{tab:brown} \textbf{Wang et al.}} & -77.83 & 1 & -81.95 & 0.9976 & -84.58 & 0.9956 & -89.39 & 0.9916 \\
    \bottomrule
    \end{tabular}
    \caption{
        Comparison of PA and $\operatorname{AFR}_{\text{P}}$ for a 
        PGD attack with $\ell_\infty$ = 16 / 255 across different adversarial 
        ratio values. The worst robustness score is emboldened for every case.
        PA displays higher consistency and discriminative power 
        across varying $\operatorname{AR}$ with respect to to 
        $\operatorname{AFR}_{\text{P}}$.
    }
    \label{tab:pa_afrpred_comparison_table}
\end{table}


These observations lead to the conclusion that PA offers a more reliable
assessment of adversarial robustness, which in general aligns with the decrease
in performance on perturbed samples, but that relies heavily on the informativeness
of the posterior distribution and the confidence in the predictions for both
original and perturbed samples. \\

\subsection{Interpretability of PA in the adversarial setting}

In light of the results obtained, the suitability of
PA in the adversarial setting has been demonstrated, but a deeper exploration of the
reason of the discrepancies between PA and the baseline robustness measures is needed
so that it can confidently be established as a model selection criterion. In particular,
we will work with the approximated expression of PA derived in Theorem \ref{thm:approximated_pa} 
and ellucidate the source of the measured robust behaviour. \\

Table \ref{tab:approx_pa_pgd_table} shows the contribution of each subset of observations to the final
approximated PA value for a PGD attack. $N_{\text{ERR}}$, $N_{\text{MIS}}$ and $N_{\text{ADV}}$ are the number of (pairs of)
contributing samples, and $\Xi_{\text{ERR}}$, $\Xi_{\text{MIS}}$ and $\Xi_{\text{ADV}}$ are
the total amount of the contribution. For reasons described earlier in this section,
the PA approximation overestimates penalizations when compared to the true value, but
relative discrepancies between models are still largely preserved and therefore the rationale
behind the discriminative power of PA, as shown in Figures \ref{fig:appendix_adversarial_approx_pa_pgd}
and \ref{fig:appendix_adversarial_approx_pa_fmn}. The parameters $2 \delta_{\text{MIS}}$ and
$\delta_{\text{ADV}}$ account for the average probability assigned to classes other than the predicted
class for misclassified original samples and misleading adversarial samples, respectively, and 
help interpret the informativeness of the distibution as well as the value of each individual 
penalization. \\

For instance, a large $2 \delta_{\text{MIS}}$ value indicates robustness to sampling randomness, as it
represents higher average uncertainty in misclassified predictions. A model with a high performance on 
test data entails a more negative penalization $\log(1 - 2 \delta_{\text{MIS}})$, for being misclassified 
samples more likely to be equivalently misclassified under adversarial perturbations, but at
the same time makes misclassifications less likely, and therefore the number of terms added to
$\Xi_{\text{MIS}}$. The existing trade-off between standard and robust generalization arises when 
following this reasoning towards the minimization of $\Xi_{\text{MIS}}$, because reducing the number of 
misclassified samples will drive $\beta^{*}$ to higher values and therefore decrease adversarial 
uncertainty $\delta_{\text{ADV}}$. As outlined before, $\delta_{\text{ADV}}$ indicates 
robustness to adversarial perturbations, as it represents the average prediction uncertainty on adversarial 
misleading samples, and entails a penalization of $\log(\delta_{\text{ADV}})$. \\

The interpretation of these terms is vitally important for the purpose of this work, as it enables
the identification of the different sources of robustness displayed by each model, and therefore
the characterization of the randomness that we will demand models to generalize to. From a general 
perspective, $\Xi_{\text{SAM}} = \Xi_{\text{ERR}} + \Xi_{\text{MIS}}$ can be understood as the lack of robustness
to sampling randomness, and $\Xi_{\text{ADV}}$ as the lack of robustness to adversarial perturbations. \\


\begin{table}[H]
    \centering
    \begin{tabular}{l|rrr|rrr}
    Defense & $N_{\text{MIS}}$ & $2 \delta_{\text{MIS}}$ & $\Xi_{\text{SAM}}$ & $N_{\text{ADV}}$ & $\delta_{\text{ADV}}$ & $\Xi_{\text{ADV}}$ \\
    \midrule
    {\color{tab:brown} \textbf{Wang et al.}} & 799 & 0.24 & -468.62 & 47 & 0.44 & -39.44 \\
    {\color{tab:blue} \textbf{Engstrom et al.}} & 1591 & 0.17 & -566.72 & 67 & 0.39 & -63.43 \\
    {\color{tab:red} \textbf{Wong et al.}} & 1562 & 0.17 & -537.25 & 90 & 0.38 & -88.98 \\
    {\color{tab:purple} \textbf{Addepalli et al.}} & 2063 & 0.21 & -877.42 & 75 & 0.46 & -58.92 \\
    {\color{tab:orange} \textbf{Undefended}} & 566 & 0.47 & -736.63 & 810 & 0.24 & -1173.55 \\
    {\color{tab:green} \textbf{Athalye et al.}} & 1915 & 0.23 & -963.85 & 747 & 0.21 & -1183.96 \\
    \bottomrule
    \end{tabular}
    \caption{
    Approximated PA contributions for a PGD attack with $\ell_\infty$ = 8/255 and
    $\operatorname{AR} = 1.0$. The number of originally misclassified and adversarially misleading
    samples is $N_{\text{MIS}} = \lfloor N (1-\tau) \rho \rfloor$ and
    $N_{\text{ADV}} = \lfloor N \tau (1-\rho) \rfloor$, respectively. 
    The penalization argument $2 \delta_{\text{ERR}}$ has not
    been included for being negligible in all cases.
    }
    \label{tab:approx_pa_pgd_table}
\end{table}

As expected, the standard generalization error term $\Xi_{\text{SAM}}$ is the one contributing most to
the PA measure in robust models, as the selected PGD attack is not very effective and can only generate
a small number of misleading samples $N_{\text{ADV}}$. The discrimination of models based exclusively 
on $\Xi_{\text{SAM}}$  is very much aligned with that of AFR (T) in all cases with the exception of the
{\color{tab:orange} \textbf{Undefended}} model, which is penalized more heavily for providing
overconfident predictions with a large number of misleading examples $N_{\text{ADV}}$ and
thus converging to a small $\beta^{*}$. This is an important realization, as it shows that even if
standard and adversarial robustness contributions can be dissociated, they are mutually dependent
and ultimately derive from the overall agreement in all predictions, regardless of the nature of the
randomness they are bound to. The generalization error to sampling randomness will be exceedigly penalized
the less robust a model is to other sources of randomness, because the optimal resolution of the hypothesis
space is reduced and the less distinction can be made between adversarial samples and outliers from
the original dataset. Further insights into this reasoning can be obtained by comparing these results
with those of the {\color{tab:green} \textbf{Athalye et al.}} model, which has a similar accuracy
on adversarial samples and a significantly worse accuracy on original samples. The fact that posterior
distributions are profoundly uninformative increases agreement in between mismatching posterior and
thus lowers penalization terms, even if more terms will be added as a consequence of the
associated decrease in performance. \\

The same reasoning can be followed to explain the discrimination made by PA between
{\color{tab:purple} \textbf{Addepalli et al.}} and the other robust models. {\color{tab:purple} \textbf{Addepalli et al.}}
experiences a comparable drop in performance, and for displaying a reduced confidence in mismatching predictions
is assigned a smaller $\Xi_{\text{SAM}}$ contribution than some of these models. Nevertheless, such 
uncertainty is also observed for original samples, which lowers accuracy on the original dataset 
and thus increases random sampling penalization $\Xi_{\text{SAM}}$. In that sense, it can be argued
that {\color{tab:purple} \textbf{Addepalli et al.}} is more robust than {\color{tab:red} \textbf{Wong et al.}}
and {\color{tab:blue} \textbf{Engstrom et al.}} to adversarial perturbations, which also stems from
the baseline AFR (P) and $\Delta$AFR values, but significantly less robust to sampling randomness.
PA weights both contributions and yields an intermediate model selection criterion. \\

Regarding adversarial robustness, we observe that $\Xi_{\text{ADV}}$ is driven by the decrase in 
performance under attack $\Delta$AFR, as $N_{\text{ADV}}$ penalization terms
are added. Nevertheless, the value of each of these terms is $\log(\delta_{\text{ADV}})$, which penalizes
models that achieve maximum posterior agreement by increasing confidence on adversarially
misleading examples. This is a clear distinctive trait with respect to accuracy-based metrics,
whose penalizations are reduced to a binary decision.


\begin{table}[H]
    \centering
    \begin{tabular}{l|rrr|rrr}
    Defense & $N_{\text{MIS}}$ & $2 \delta_{\text{MIS}}$ & $\Xi_{\text{SAM}}$ & $N_{\text{ADV}}$ & $\delta_{\text{ADV}}$ & $\Xi_{\text{ADV}}$ \\
    \midrule
    {\color{tab:purple} \textbf{Addepalli et al.}} & 1507 & 0.52 & -1910.69 & 2187 & 0.28 & -2788.89 \\
    {\color{tab:red} \textbf{Wong et al.}} & 1032 & 0.46 & -1125.53 & 2920 & 0.27 & -3844.40 \\
    {\color{tab:blue} \textbf{Engstrom et al.}} & 1125 & 0.72 & -2469.65 & 2505 & 0.32 & -2847.99 \\
    {\color{tab:brown} \textbf{Wang et al.}} & 435 & 0.82 & -1599.08 & 4215 & 0.33 & -4637.45 \\
    {\color{tab:orange} \textbf{Undefended}} & 412 & 0.56 & -704.58 & 3132 & 0.29 & -3906.55 \\
    {\color{tab:green} \textbf{Athalye et al.}} & 859 & 0.57 & -2054.23 & 4679 & 0.43 & -3955.43 \\
    \bottomrule
    \end{tabular}
    \caption{
    Approximated PA contributions for a FMN attack with $\operatorname{AR} = 1.0$. The number of 
    originally misclassified and adversarially misleading
    samples is $N_{\text{MIS}} = \lfloor N (1-\tau) \rho \rfloor$ and
    $N_{\text{ADV}} = \lfloor N \tau (1-\rho) \rfloor$, respectively. 
    The penalization argument $2 \delta_{\text{ERR}}$ has not
    been included for being negligible in all cases with the exception of the 
    {\color{tab:green} \textbf{Athalye et al.}} model, which amounts to 0.36. 
    }
    \label{tab:approx_pa_fmn_table}
    \end{table}

In contrast with the PGD case, the FMN attack is much more effective and $N_{\text{ADV}} > N_{\text{MIS}}$
in all cases, which makes the adversarial contribution $\Xi_{\text{ADV}}$ more relevant in the
overall robustness assessment. The discrepancy observed between PA and
accuracy-based metrics for the {\color{tab:red} \textbf{Wong et al.}} model can also be explained 
in these terms, as it is the least penalized by $\Xi_{\text{SAM}}$ among robust models due to its
superior accuracy. In the context of effective attacks, predictive certainty on
original samples is highly rewarded because lower overall agreement makes sampling penalization terms
$log(1 - 2 \delta_{\text{MIS}})$ more negative. \\

Overall, this approximation shows that the final PA value stems from a combination of 
standard and adversarial generalization error, which are normally obtained independently through
different accuracy-based metrics, and leads to the realization that PA provides an intermediate
assessment in between AFR measures. An analogous combined metric weighting
accuracy and $\Delta$AFR would not be equiparable to PA, given that these weights would be arbitrary
and would not adjust to the particularities of each model. For instance, the 
{\color{tab:orange} \textbf{Undefended}} model should be mostly penalized on the basis of its
robustness to adversarial examples, whereas {\color{tab:purple} \textbf{Addepalli et al.}}
model should be mostly penalized for its lack of robustness to sampling randomness in the
original data. These considerations are fundamental in the covariate shift setting, as
different models with different defensive or invariant feature learning strategies will
navigate the generalization-complexity trade-off in a different way. \\

As a conclusion to our analysis, other kinds of metrics have been contrasted to determine whether they
are able to provide a reliable assessment of adversarial and sampling robustness that is comparable to
the baseline accuracy-based measures. These include confidence-based measurements,
such as the Kullback-Leibler (KL) divergence and the Wasserstein distance between posterior
distributions, and typical distance measures on the feature space of the model, namely 
through dataset cosine similarity, distance between centroids, and other cluster-based approaches 
such as the maximum mean discrepancy or the Fr\'{e}chet inception distance (FID). This last one is 
particularly interesting, as it is a widely used metric in the generative adversarial network 
literature and thus fits intuitively well into our setting. \\

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/DIFF_PGD_0.1255.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/FID_barplot_0.1255.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/W_barplot_0.1255.png}
    \end{subfigure}

    \vspace{1em}

    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/DIFF_FMN.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/FID_barplot_FMN.png}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/adversarial/W_barplot_FMN.png}
    \end{subfigure}

    \caption{
    The two metrics considered are FID, which amounts to the group-based dissimilarity 
    in the feature space, and Wasserstein distance, which measures the average distance between 
    probability distributions. 
    }
    \label{fig:adv_metric_comparison}
\end{figure}

Figure \ref{fig:adv_metric_comparison} displays some of these metrics for
PGD and FMN attacks, together with a condensed visualization of the average 
probability gap existing amidst predictions on original and adversarial examples. As
we can see, the discriminative power of these metrics stems from the overall difference 
between posteriors at $\beta = 1$. This result is not surprising, since the discriminant function 
of the classifier is constituted by a small subset of layers in the network, and is not expected 
to represent a complex discrimination that cannot be inferred from the distribution 
of samples in the feature space. As expected, probability-based metrics tend to concide in 
their assessment in general terms, and feature-space-based metrics as well. Besides, it is also not
surprising that feature space distances are more sensitive to the success rate of the attack, 
given that the inductive bias remains constant under any condition, whereas confidence-based metrics
are susceptible to the overall shift in the posterior, which in general favours non-informative
posteriors. A deeper insight into the evolution of these metrics for PGD and FMN attacks under increasing
adversarial ratio can be found in Figures \ref{fig:comparison_prob_metrics} and 
\ref{fig:comparison_feat_metrics}. \\

The results obtained in this section showcase the rationale behind the maximization of posterior
agreement, which can be thought of a surrogate version of the maximization of the mutual information
between original and perturbed datasets
\cite{buhmannDataScienceAlgorithms2022}. From an information-theoretic perspective, 
the maximization of mutual information effectively
distillates the information contained in the posterior distributions to that 
which is relevant for robustness assessment. This property is the source of the 
discriminative power displayed by PA and constitutes a fundamental difference
with respect to the baseline accuracy-based metrics considered.


\section{Domain generalization setting}\label{results_domain_generalization}

Following the analysis conducted in the previous section, the discriminability of PA
will be now explored in the domain generalization setting, which is a priori more convenient
for PA for being accuracy-based metrics less informative in this context. This is because
we are ultimately assessing the quality of the inductive bias of the model
by its ability to generalize to target (i.e. unseen) domains. In this sense, the additional
insight and discriminability exhibited by PA is expected to be more relevant for the selection of
models that perform well not only on unseen data, but also on unseen data that shares limited 
features with the training data. Under these conditions, the overlap between posteriors is more
informative than simply matching predictions (e.g. $\operatorname{AFR}_{\text{P}}$), because 
significant disagreement in
the remaining classes indicates vulnerability to distribution shifts present in
the source domains, which implies vulnerability to target domains as well. This is
a fundamental difference with respect to the adversarial setting, in which the nature of the
perturbation made posteriors less relevant for the robustness assessment.\\

This section will not address epoch-wise model selection, but will focus instead on 
the evaluation of the generalization capabilities of different learning algorithms under increasing 
levels of distribution shift, by computing the posterior agreement between source environments for 
models achieving maximum validation accuracy. More specifically, a baseline vanilla ERM algorithm will be 
used to train a ResNet18 model and will be compared with two robust 
learners, namely invariant risk minimization (IRM) and selective
augmentation (LISA), both introduced in Section \ref{sec:robust_learners}. Results should
ellucidate whether PA is able to discern datasets subjected to different levels of domain shift and
whether models achieving highest PA scores perform better on new domains. \\

Experiments will be performed by means of the DiagViB-6 dataset
framework
\cite{euligDiagViB6DiagnosticBenchmark2021}, 
which comprises MNIST images of size 128x128 within an augmentation pipeline enabling
the modification of six specific image factors: shape, hue, lightness, position,
scale and texture. Several variations in the \texttt{diagvibsix}
library\footnote{https://github.com/viictorjimenezzz/diagvibsix/tree/librarization}
have been implemented with the purpose 
of this project so that datasets can be built with a specific configuration of factors for each sample, which 
allows for a wide range of experiments in the data shift assessment and model selection settings. In
an analogous way to the adversarial case, datasets will be incrementally perturbed by including only
a fraction of the shifted samples, which in this context we will call shift ratio (SR).\\

Following the notation introduced in Section \ref{sec:domain_generalization_setting}, Definition 
\ref{def:shifted_factors_experiment} provides a characterization of source and target domains and
the randomness entailed by each dataset. The control over these aspects is the rationale behind
this experimental setup, since it is through synthetic image manipulation that we can maximize
invariant feature learning possibilities during training while providing optimal robustness
assessment conditions in validation and testing. Since changes in image factors can be independently 
introduced to each sample, the shifted dataset contains the same samples and in the same order
as the original dataset, thus removing sampling randomness contributions from the robustness score. 
Table \ref{tab:data_shift_table} stipulates the specific factors conforming each environment in this
experiment and Figure \ref{fig:data_shift_images} illustrates them with some examples.

\begin{definition}[Shifted factors experiment]\label{def:shifted_factors_experiment}
    The classification task involves the prediction of the shape factor (i.e. the digit)
    of handwritten fours and nines from the MNIST dataset. In particular, source and target
    domains are generated as follows:

    $$
    \begin{aligned}
        &\mathcal{S} = \{ X_0, X_1\},\\
        &\mathcal{T} = \{ X_1, X_2, X_3, X_4, X_5\},
    \end{aligned}
    $$

    where $X_j$ represents the random variable associated to domain $j$, 
    being $j$ the number of shifted factors with respect to the original MNIST sample. Datasets are generated by considering
    four different realizations of the experiment, namely $\tau_0^{\text{train}}$, $\tau_1^{\text{train}}$, $\tau^{\text{val}}$
    and $\tau^{\text{test}}$, each sampling from disjoint subsets of MNIST. Following the notation introduced
    in Chapter \ref{sec:experimental_setup}, we can define:
    $$
    \begin{aligned}
        &D^{\text{train}} = \{\bm{x}_0^{\text{train}}, \bm{x}_1^{\text{train}}\}, \; \text{where } \bm{x}_j := \bm{x}_j \circ \tau_j^{\text{train}}, \;j = 0,1 \\
        &D^{\text{val}} = \{\bm{x}_0^{\text{val}}, \bm{x}_1^{\text{val}}\}, \; \text{where } \bm{x}_j := \bm{x}_j \circ \tau^{\text{val}}, \;j=0,1 \\
        &D_j^{\text{test}} = \{\bm{x}_j^{\text{test}}\}, \; \text{where } \bm{x}_j^{\text{test}} := \bm{x}_j^{\text{test}} \circ \tau^{\text{test}}, \;j = 1,\dots,5
    \end{aligned}
    $$

    In this way, only training data is subject to both sampling randomness ($\tau_0^{\text{train}} \neq \tau_1^{\text{train}}$)
    and domain shift ($X_0 \nsim X_1$), emulating the conditions of real-world sampling experiments.
    In contrast, validation and testing datasets entail each a single noise instantiation, 
    which means that distribution shift is the only accountable source of randomness.
    Overall, two sets of $40\,000$ images for training, two sets of $20\,000$ 
    images for validation, and six sets of $10\,000$ images for testing are generated. 

\end{definition}

\begin{table}[H]
    \centering
    \begin{tabular}{l|c|c|c|c|c|c}
    \# Shift Factors & 0 & 1 & 2 & 3 & 4 & 5 \\
    \midrule
    Hue & red & \textbf{blue} & blue & blue & blue & blue \\
    Lightness & dark & dark & \textbf{bright} & bright & bright & bright \\
    Position  & CC & CC & CC & \textbf{LC} & LC & LC \\
    Scale  & normal & normal & normal & normal & \textbf{large} & large \\
    Texture & blank & blank & blank & blank & blank & \textbf{tiles} \\
    \textit{Shape} & \textit{4,9} &  \textit{4,9} &  \textit{4,9} & \textit{4,9} & \textit{4,9} & \textit{4,9} \\
    \bottomrule
    \end{tabular}
    \caption{
    Image factors associated to each of the environments considered in this experiment. CC and LC account
    for 'centered center' and 'centered low', respectively.
    }
    \label{tab:data_shift_table}
    \end{table}


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[height=2cm]{img/results_discussion/datashift/dsimages/train_collage.png}
        \caption*{Train}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[height=2cm]{img/results_discussion/datashift/dsimages/val_collage.png}
        \caption*{Validation}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[b]{0.6\textwidth}
        \centering
        \includegraphics[height=2cm]{img/results_discussion/datashift/dsimages/test_collage2.png}
        \caption*{Test (1 - 5)}
    \end{subfigure}
    \caption{
    Illustration of the training, validation and test datasets. Samples for each
    training environment belong to different MNIST subsets, whereas samples of
    validation and test are corresponding.
    }
    \label{fig:data_shift_images}
\end{figure}


Results obtained in this setup show that PA succeeds at discriminating the 
different models by their predictive response under increasing number of shifted samples and 
under increasing shift power. In particular, ERM can be identified to be non-robust by the fact 
that its score is maximum for the first shifted factor, but decays rapidly to the minimum value 
after the second factor. In contrast, IRM and LISA show a reduced rate of decay and even display a
slight increase in performance for the last shift factor. \\

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/datashift/shift_ratio=0.200.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/datashift/shift_ratio=0.400.pdf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/datashift/shift_ratio=0.600.pdf}
    \end{subfigure}

    \vspace{1em}

    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/datashift/shift_ratio=0.800.pdf}
    \end{subfigure}
    \hspace{13pt}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/datashift/shift_ratio=1.000.pdf}
    \end{subfigure}

    \caption{Evolution of PA under increasing levels of shift power. Weights maximizing validation
    accuracy were selected for ERM, IRM and LISA algorithms. Results for incremental presence of shifted
    samples indicate that PA is able to differentiate weak and robust models.
    }
    \label{fig:six_figures}
\end{figure}

The unexpected increase in PA after four shifted factors seems
inconsistent with the alleged non-increasing behaviour of PA, as per
Section \ref{sec:robustness_to_covariate_shift}. Nevertheless, this phenomenon 
only highlights that robustness does not stem from the data generation process but instead 
from the latent representation of the model and the features selected for the construction 
of its inductive bias, as discussed in the introductive chapter. In this particular case, 
Table \ref{tab:CS_shift} shows that
there is a clear discontinuity in the feature representation of the data when the texture
factor is shifted, which results in a different discriminator function that ultimately
leads to a predictive behaviour that aligns slightly better with the original predictions. \\

\begin{table}[H]
    \centering
    \begin{tabular}{l|c|c|c|c|c}
    \# Shift Factors & 1 & 2 & 3 & 4 & 5 \\
    \midrule
    ERM & 0.9978 & 0.9303 & 0.9562 & 0.9561 & \textbf{0.6661} \\
    IRM & 0.9967 & 0.9018 & 0.9296 & 0.9374 & \textbf{0.5585} \\
    LISA  & 0.9980 & 0.9431 & 0.9431 & 0.9641 & \textbf{0.7130} \\
    \bottomrule
    \end{tabular}
    \caption{
    Pairwise cosine similarity between feature space representations of original and
    augmented images, for each of the shifted datasets. The abrupt decrease in similarity
    for the fifth environment indicates a discontinuity in the feature representation of images,
    which leads to non-comparable predictive outcomes.
    }
    \label{tab:CS_shift}
    \end{table}


% - Here explain that PA can also be approximated by considering $\beta$ a fixed beta and evaluating
% in many environments. This validity of this approximation will be higher the lower is the decrease
% in performance of the model under increasing shift power. In any case, the assessment won't be optimal
% but can be used to spare computational ressources. Figure \ref{fig:datashift_eval_pa} shows the evaluated PA (i.e. $\beta^{*}$ computed only for the
% first two environments) for each model against the standard accuracy displayed during the whole
% optimization process. \\


% \begin{figure}[H]
%     \centering
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{img/results_discussion/datashift/paper_oracle_all_erm.png}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{img/results_discussion/datashift/paper_PA_all_erm.png}
%     \end{subfigure}

%     \vspace{1em}

%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{img/results_discussion/datashift/paper_oracle_all_irm.png}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{img/results_discussion/datashift/paper_PA_all_irm.png}
%     \end{subfigure}

%     \caption{PA evaluation.}
%     \label{fig:datashift_eval_pa}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.7\textwidth]{img/results_discussion/datashift/model_selection.png}
%     \caption{Model selection capabilities.}
%     \label{fig:model_selection_capabilities}
% \end{figure}

As shown in Figure \ref{fig:datashift_selection}, PA-based model selection behaves differently in robust
and non-robust algorithms. Following the reasoning derived in previous sections, it is likely that the PA
assessment on ERM is mostly driven by standard generalization error, due to the fact that ERM is agnostic
of the environment to which each sample belongs to and therefore to the accountable source of randomness represented
by the environment shift. The risk minimization problem under these conditions should give rise to less informative
predictive outcomes, given that the domain-invariant features associated with the task are less accessible, and
thus reduce the penalization weight of mismatching samples. This intuition is supported by the fact that ERM
optimization yields models that performs worse in all test datasets than the ones obtained through IRM,
as can be seen in Figure \ref{fig:datashift_eval_pa}. Besides, results show that the selected model for ERM 
performs better an all metrics for the first test environment, which contains
the same factor configuration as one of the validation environments, and maintains or slightly decreases its performance
for increasing levels of shift. In addition, the PA-selected model appears to encode a slight variation of the 
decision rule from that of the accuracy-selected model, given that samples near to the decision boundary are now
more likely classified as nines than fours, which increases specificity as much as it decreases sensitivity. \\

Regarding PA-based model selection in robust learners, we observe the opposite behaviour. Since models are more likely
to represent domain-invariant features, each with its distinctive strategy, predictive outcomes are expected to
be more informative and thus increase the penalization contribution of mismatching samples, which will be the ones
containing domain-invariant features that are not sufficiently considered in the inductive bias of the model. PA
model selection under these conditions is thus more likely to favor sets of weights that perform better under
increasing levels of shift, at the expense of losing predictive power on the environments in which it operated. 
Results align with this interpretation, as we observe a notable increase in performance across all shifted datasets
with respect to the accuracy-selected model, especially in the LISA case, at the expense of significantly decreasing
performance on the first environment.\\


\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/datashift/paper_selection_ppred=1.0_met=acc.png}
    \end{subfigure}

    \begin{subfigure}[b]{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/datashift/paper_selection_ppred=1.0_met=sensitivity.png}
    \end{subfigure}

    \begin{subfigure}[b]{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/results_discussion/datashift/paper_selection_ppred=1.0_met=specificity.png}
    \end{subfigure}
    \caption{
    Accuracy, sensitivity and precision displayed by sets of ResNet18 weights on
    shifted test datasets, obtained through ERM, IRM and LISA
    training procedures. Accuracy-based selection is compared to PA-based selection, both
    operating with a validation dataset composed of samples of environments 0 and 1.
    }
    \label{fig:datashift_selection}
\end{figure}


\cleardoublepage
