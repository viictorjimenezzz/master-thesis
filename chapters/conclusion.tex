\chapter{Conclusions}\label{sec:conclusions}

The objectives of this thesis centered on assessing the suitability of the posterior agreement framework 
as a robust model selection criterion within the context of deep learning for image classification tasks. The work entailed 
deriving and implementing an operative version of posterior agreement for discrete hypothesis classes, exploring its 
properties and comparing its discriminative power against that of baseline accuracy-based metrics. Experiments in adversarial 
and domain generalization settings were conducted to explore the source of the robustness measure provided by PA
and its capabilities for selecting robustness-fostering algorithms. Additionally, results were provided on the 
effectiveness of PA as an early-stopping criterion for epoch-wise model selection, with experiments designed to 
assess its validity in both controlled syntetic environments and real-world scenarios, ultimately culminating in 
performance measures in benchmark out-of-distribution datasets. These objectives guided the research process, shaping the 
experiments and analyses that ultimately conform the conclusions of this work. \\



\begin{itemize}
    \item Analyze the robustness challenge in image classification tasks. Mention the three different
    sources of randomness, and the trade-off existing when making kmodels robust to them, which
    arises from a fundamental difference in the inductive bias of the model. This thesis introduces
    notation for a probability theory.\\
    \item The trade-off can be reformulated from an information theoretic perspective, as an estimation
    of the information content of the data, and the deried resolution over the hypothesis space. A robust learner
    is such that it correctly estimates the information content of the data and results in a hypothesis space that
    is stable to the randomness associated with the data generation process.  \\
    \item The complexity of a model can be derived from the normalized description length value, and thus generalization
    error between  pair of datasets can be defined by averaging it over the hypothesis space using the probabikity distribution
    of the first dataset. \\
    \item A lower bound for the generalization error can be obtainedd, leading to the expression of PA. An operative version of
    posterior agreement for finite hypothesis spaces is introduced and accessible using an efficient implementation that is easily
    to use by the ML community. \\
    \item Results were obtained 
\end{itemize}


What you did:
- Probability theory formulation of the covariate shift setting and the three sources of robustness.
- Derivation of the posterior agreement kernel for finite hypothesis classes. Proof of its properties, in particular convexity.
- Efficient implementation of the PA metric.
- Definition of a robustness metric, which is fundamentally different from a performance metric, and proof that PA
does comply with the required properties.
- Proved the suitability of PA as a robustness measure in the adversarial setting. In this setting, a model is considered to
be more robust the better it performs in adversarial samples. In this sense, the robustness measure provided by PA was approximated
and broken down to show that it accounts for both sampling randomness and randomness to perturbations, efficiently combining
the usual standard accuracy-based metrics, but providing higher discriminability and consistency over different levels of
attack power and samples ratio.
- Proved that PA provides a consistent algorithm selection assessment based on its predictive power under distribution shifts.


Results have been outlined a deductive way, starting with an 
empirical exploration of the robustness properties of the PA kernel and culminating
with performance results in benchmark datasets.

\cleardoublepage
